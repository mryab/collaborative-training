{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colab_starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mryab/collaborative-training/blob/auth/colab_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQIcM112KfI"
      },
      "source": [
        "<center><img src=\"https://i.imgur.com/FHMoW3N.png\" width=360px><br><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collaborative training <sup>v0.9 alpha</sup></b></center>\n",
        "\n",
        "\n",
        "This notebook will use local or colab GPU to help train ALBERT-large collaboratively. Your instance will compute gradients and exchange them with a bunch of volunteers around the world. We explain how it works at the bottom. But for now, please run all cells :)\n",
        "\n",
        "To start training, you will need to login to your huggingface account, please fill in the prompts as in the example below (replace `robot-bengali` with your username):\n",
        "\n",
        "![img](https://i.imgur.com/txuWbJi.png)\n",
        "\n",
        "This is a test run to root out any issues before the main event. The run will be terminated by __23:59 11 may GMT+0__. Please do not run colab notebooks from multiple google accounts: google doesn't like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlL47uTK7DN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527712f4-3216-4acf-ca45-98bf01b35f9a"
      },
      "source": [
        "experiment_name = \"bengali_test_1_auth\"\n",
        "hivemind_version = \"bengali_test_1_auth\"\n",
        "collaborative_training_version = \"auth\" \n",
        "syslog_host = '18.191.120.93'\n",
        "\n",
        "!echo \"Installing dependencies...\"\n",
        "!pip install git+https://github.com/learning-at-home/hivemind.git@{hivemind_version} >> install.log 2>&1\n",
        "!git clone https://github.com/mryab/collaborative-training -b {collaborative_training_version} >> install.log 2>&1\n",
        "!cd collaborative-training && pip install -r requirements.txt >> install.log 2>&1 && cd ..\n",
        "%cd ./collaborative-training\n",
        "\n",
        "import shlex\n",
        "from getpass import getpass\n",
        "import torch\n",
        "from runner import run_with_logging\n",
        "assert torch.cuda.is_available(), \"GPU device not found. If running in colab, please retry in a few minutes.\"\n",
        "device_name = torch.cuda.get_device_name(0)\n",
        "microbatch_size = 4 if 'T4' in device_name or 'P100' in device_name else 1\n",
        "print(f\"Running with device {device_name}, local batch size = {microbatch_size}\")\n",
        "\n",
        "username = shlex.quote(input('Huggingface login: '))\n",
        "password = shlex.quote(getpass('Huggingface password: '))\n",
        "\n",
        "command = f\"\"\"ulimit -n 4096 && HIVEMIND_THREADS=256 HF_USERNAME={username} HF_PASSWORD={password} python ./run_trainer.py \\\n",
        " --client_mode --averaging_expiration 10 --statistics_expiration 120 \\\n",
        " --batch_size_lead 200 --per_device_train_batch_size {microbatch_size} --gradient_accumulation_steps 1 \\\n",
        " --logging_first_step --logging_steps 100 --run_name {username}  --output_dir ./outputs --overwrite_output_dir --logging_dir ./logs \\\n",
        " --experiment_prefix {experiment_name} --seed 42\"\"\"\n",
        "run_with_logging(command, syslog_host, wandb_login=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "/content/collaborative-training\n",
            "Running with device Tesla T4, local batch size = 4\n",
            "Huggingface login: robot-bengali\n",
            "Huggingface password: ··········\n",
            "2021-05-07 09:19:00.512895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n",
            "Downloading:   0%|          | 0.00/685 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 685/685 [00:00<00:00, 1.00MB/s]\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "Downloading:   0%|          | 0.00/1.16M [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 1.16M/1.16M [00:00<00:00, 56.4MB/s]\n",
            "Downloading:   0%|          | 0.00/244 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 244/244 [00:00<00:00, 397kB/s]\n",
            "Downloading:   0%|          | 0.00/567 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 567/567 [00:00<00:00, 992kB/s]\n",
            "punkt not found. downloading...\n",
            "[2021/05/07 09:19:16.518][INFO][client.averaging._run:186] The averager running in an experimental client mode, please report any bugs.\n",
            "Downloading:   0%|          | 0.00/2.38k [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 2.38k/2.38k [00:00<00:00, 2.65MB/s]\n",
            "Downloading:   0%|          | 0.00/5.58k [00:00<?, ?B/s]\n",
            "Downloading: 14.7kB [00:00, 13.3MB/s]\n",
            "Downloading:   0%|          | 0.00/359k [00:00<?, ?B/s]\n",
            "Downloading: 3.07MB [00:00, 138MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:894: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "wandb: Currently logged in as: yhn112 (use `wandb login --relogin` to force relogin)\n",
            "2021-05-07 09:19:20.197904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "wandb: Tracking run with wandb version 0.10.29\n",
            "wandb: Syncing run robot-bengali\n",
            "wandb: ⭐️ View project at https://wandb.ai/yhn112/Test-Bengali-Run\n",
            "wandb: 🚀 View run at https://wandb.ai/yhn112/Test-Bengali-Run/runs/3hvlofjn\n",
            "wandb: Run data is saved locally in /content/collaborative-training/wandb/run-20210507_091918-3hvlofjn\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "[2021/05/07 09:19:22.362][INFO][client.averaging._load_state_from_peers:438] Downloading parameters from peer ipv4:18.222.113.188:35557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93KCg3RZzEks"
      },
      "source": [
        "### What's up next?\n",
        "* Check the training progress on public learning curves: https://wandb.ai/yhn112/Demo-run-2/runs/2tqiiq40\n",
        "* Run a second GPU session with kaggle notebooks: **TBA**\n",
        "* See [this tutorial](https://github.com/learning-at-home/hivemind/tree/master/examples/albert) on how to start your own collaborative runs!\n",
        "\n",
        "\n",
        "_Co-created by [yhn112](https://github.com/yhn112), [leshanbog](https://github.com/leshanbog), [foksly](https://github.com/foksly) and [borzunov](https://github.com/borzunov) from [hivemind](https://github.com/learning-at-home/hivemind) (YSDA), [lhoestq](https://github.com/lhoestq), [SaulLu](https://github.com/SaulLu) and [stas00@](https://github.com/stas00) from [huggingface](http://huggingface.co)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUQ_j06kS6pb"
      },
      "source": [
        "### How it works\n",
        "\n",
        "Since peers can join and leave at any time, we can't use global [Ring All-Reduce](https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da) for averaging: a single missing peer can break the entire protocol. Instead, peers dynamically assemble into small groups and run all-reduce within each group. Consider an example with 9 GPUs:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/QcD1mfG.png\" width=360px><br>\n",
        "The All-Reduce protocol within group can be Ring-AllReduce, but we use a simpler all-to-all algorithm known as butterfly-like all-reduce.<br>\n",
        "<img src=\"https://i.imgur.com/ewq3vS6.png\" width=380px><br>\n",
        "After each successful round, participants shuffle around and find new groups:<br>\n",
        "<img src=\"https://i.imgur.com/dexNCL3.png\" width=350px>\n",
        "\n",
        "If one of the peers fails to do his part, it will only affect his local group, and only for a single round.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/RBmElUi.png\" width=340px>\n",
        "\n",
        "Afterwards, peers from the failed group will find new groupmates according to the [moshpit algorithm](https://arxiv.org/abs/2103.03239).\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "If you want to learn more and even host your own collaborative experiments, take a look at the [hivemind library](https://github.com/learning-at-home/hivemind/) or the [Moshpit-SGD paper](https://arxiv.org/pdf/2103.03239.pdf).\n",
        "\n",
        "\n"
      ]
    }
  ]
}
