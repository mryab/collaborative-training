{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colab_starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/justheuristic/60fdfe5c90c053a93c00f950a1abd0da/collaborative-training-v0-14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQIcM112KfI"
      },
      "source": [
        "<center><img src=\"https://i.imgur.com/FHMoW3N.png\" width=360px><br><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collaborative training <sup>v0.9 alpha</sup></b></center>\n",
        "\n",
        "\n",
        "This notebook will use local or colab GPU to help train ALBERT-large collaboratively. Your instance will compute gradients and exchange them with a bunch of volunteers around the world. We explain how it works at the bottom. But for now, please run all cells :)\n",
        "\n",
        "This is a test run to root out any issues before the main event. The run will be terminated by __23:59 11 may GMT+0__. Please do not run colab notebooks from multiple google accounts: google doesn't like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlL47uTK7DN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73324622-4f96-4094-fec5-cb789e913ea1"
      },
      "source": [
        "experiment_name = \"bengali_test_1\"\n",
        "hivemind_version = \"bengali_test_1\"\n",
        "collaborative_training_version = \"main\" \n",
        "coordinator_ip = '18.217.249.202'\n",
        "coordinator_port = 31337\n",
        "\n",
        "!echo \"Installing dependencies...\"\n",
        "!pip install git+https://github.com/learning-at-home/hivemind.git@{hivemind_version} >> install.log 2>&1\n",
        "!git clone https://github.com/mryab/collaborative-training -b {collaborative_training_version} >> install.log 2>&1\n",
        "!cd collaborative-training && pip install -r requirements.txt >> install.log 2>&1 && cd ..\n",
        "%cd ./collaborative-training\n",
        "\n",
        "import torch\n",
        "from runner import run_with_logging\n",
        "assert torch.cuda.is_available(), \"GPU device not found. If running in colab, please retry in a few minutes.\"\n",
        "device_name = torch.cuda.get_device_name(0)\n",
        "microbatch_size = 4 if 'T4' in device_name or 'P100' in device_name else 1\n",
        "print(f\"Running with device {device_name}, local batch size = {microbatch_size}\")\n",
        "\n",
        "import uuid\n",
        "wandb_run_name = str(uuid.uuid4())\n",
        "\n",
        "command = f\"\"\"ulimit -n 4096 && HIVEMIND_THREADS=256 python ./run_trainer.py \\\n",
        " --client_mode --initial_peers {coordinator_ip}:{coordinator_port} --averaging_expiration 10 --statistics_expiration 120 \\\n",
        " --batch_size_lead 200 --per_device_train_batch_size {microbatch_size} --gradient_accumulation_steps 1 \\\n",
        " --logging_first_step --logging_steps 100 --run_name {wandb_run_name}  --output_dir ./outputs --overwrite_output_dir --logging_dir ./logs \\\n",
        " --experiment_prefix {experiment_name} --seed 42\"\"\"\n",
        "run_with_logging(command, coordinator_ip, wandb_login=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 07:39:03.799888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n",
            "05/07/2021 07:39:07 - WARN - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "05/07/2021 07:39:07 - INFO - __main__ -   Training/evaluation parameters AlbertTrainingArguments(output_dir='./outputs', overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.00176, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1000000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=5000, logging_dir='./logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=100, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=2, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O2', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='4f6e6d18-ac75-4101-82e4-0f1729da4209', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, mp_parameters='', seq_length=512, clamp_value=10000.0)\n",
            "05/07/2021 07:39:07 - INFO - filelock -   Lock 140517081680272 acquired on ./cache-data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e.lock\n",
            "Downloading:   0%|          | 0.00/685 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 685/685 [00:00<00:00, 578kB/s]\n",
            "05/07/2021 07:39:07 - INFO - filelock -   Lock 140517081680272 released on ./cache-data/4f6b2675253400aebc8ab1673ec27add127daf362af404ae81bfc57880c04d85.044d033374cf383119201e83a30fc592581d33c3833ac0675f55b2765509ce4e.lock\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "05/07/2021 07:39:08 - INFO - filelock -   Lock 140517081772240 acquired on /root/.cache/huggingface/transformers/258992b1f6ebcb0d6c2254faff836680f87147678a42efdd55214d43391ac3e9.c0da86b7c5a46895bdf622f113f12d1c02981799993d291e60d8794eeb53122d.lock\n",
            "Downloading:   0%|          | 0.00/1.16M [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 1.16M/1.16M [00:00<00:00, 27.5MB/s]\n",
            "05/07/2021 07:39:08 - INFO - filelock -   Lock 140517081772240 released on /root/.cache/huggingface/transformers/258992b1f6ebcb0d6c2254faff836680f87147678a42efdd55214d43391ac3e9.c0da86b7c5a46895bdf622f113f12d1c02981799993d291e60d8794eeb53122d.lock\n",
            "05/07/2021 07:39:08 - INFO - filelock -   Lock 140517081722832 acquired on /root/.cache/huggingface/transformers/6ea0433beb3f853716428bd48d5e274d982f627b610f5a1aa12458416a7faede.15ed5b79b197b4fcc5f3f80b2ee89a5a3ad708dbd076575cd22cffd9e1a56284.lock\n",
            "Downloading:   0%|          | 0.00/244 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 244/244 [00:00<00:00, 206kB/s]\n",
            "05/07/2021 07:39:09 - INFO - filelock -   Lock 140517081722832 released on /root/.cache/huggingface/transformers/6ea0433beb3f853716428bd48d5e274d982f627b610f5a1aa12458416a7faede.15ed5b79b197b4fcc5f3f80b2ee89a5a3ad708dbd076575cd22cffd9e1a56284.lock\n",
            "05/07/2021 07:39:09 - INFO - filelock -   Lock 140517081798608 acquired on /root/.cache/huggingface/transformers/5326384db559384da5cc3b213b2af399a276c22529dfb5e9f001ee228bc3f975.bf161c7757756d24f3038034c4c484c63e2728ac6eca2b7ca794412611319f01.lock\n",
            "Downloading:   0%|          | 0.00/567 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 567/567 [00:00<00:00, 347kB/s]\n",
            "05/07/2021 07:39:09 - INFO - filelock -   Lock 140517081798608 released on /root/.cache/huggingface/transformers/5326384db559384da5cc3b213b2af399a276c22529dfb5e9f001ee228bc3f975.bf161c7757756d24f3038034c4c484c63e2728ac6eca2b7ca794412611319f01.lock\n",
            "05/07/2021 07:39:09 - INFO - __main__ -   Checkpoint dir outputs, contents []\n",
            "05/07/2021 07:39:09 - INFO - __main__ -   Training from scratch\n",
            "punkt not found. downloading...\n",
            "[2021/05/07 07:39:23.357][INFO][client.averaging._run:186] The averager running in an experimental client mode, please report any bugs.\n",
            "Downloading:   0%|          | 0.00/2.38k [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 2.38k/2.38k [00:00<00:00, 2.11MB/s]\n",
            "05/07/2021 07:39:25 - WARN - datasets.builder -   Using custom data configuration default\n",
            "Downloading:   0%|          | 0.00/5.58k [00:00<?, ?B/s]\n",
            "Downloading: 14.7kB [00:00, 10.3MB/s]\n",
            "Downloading:   0%|          | 0.00/359k [00:00<?, ?B/s]\n",
            "Downloading: 3.07MB [00:00, 83.6MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:894: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "wandb: Currently logged in as: yhn112 (use `wandb login --relogin` to force relogin)\n",
            "2021-05-07 07:39:29.809601: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "wandb: Tracking run with wandb version 0.10.29\n",
            "wandb: Syncing run 4f6e6d18-ac75-4101-82e4-0f1729da4209\n",
            "wandb: ⭐️ View project at https://wandb.ai/yhn112/Test%20Bengali%20Run\n",
            "wandb: 🚀 View run at https://wandb.ai/yhn112/Test%20Bengali%20Run/runs/x7nzfkgh\n",
            "wandb: Run data is saved locally in /content/collaborative-training/wandb/run-20210507_073928-x7nzfkgh\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "05/07/2021 07:39:31 - WARN - __main__ -   Loading state from peers\n",
            "[2021/05/07 07:39:32.514][INFO][client.averaging._load_state_from_peers:438] Downloading parameters from peer ipv4:3.129.247.222:37487\n",
            "05/07/2021 07:39:39 - INFO - streaming_dataset -   Pre-fetching training samples...\n",
            "05/07/2021 07:39:39 - INFO - datasets_modules.datasets.wikipedia_bn.9a39a22d1ddc62908bed998bdceb29e0c6b3f944bc62b46906783c0bfc703220.wikipedia_bn -   generating examples from = https://huggingface.co/datasets/lhoestq/wikipedia_bn/resolve/main/data/20210320/shard-00007-of-00010.parquet\n",
            "05/07/2021 07:39:40 - INFO - datasets_modules.datasets.oscar.17c59f25bedf9bdf1866f3d7931725614001fd9ea063b0dbe926605fbf23bdc4.oscar -   generating examples from = https://s3.amazonaws.com/datasets.huggingface.co/oscar/1.0/unshuffled/deduplicated/bn/bn_part_3.txt.gz\n",
            "05/07/2021 07:39:44 - INFO - streaming_dataset -   Began iterating minibatches!\n",
            "05/07/2021 07:39:46 - INFO - __main__ -   Step 168\n",
            "05/07/2021 07:39:46 - INFO - __main__ -   Your current contribution: 4 samples\n",
            "05/07/2021 07:39:46 - INFO - __main__ -   Loss of your model: 10.9221\n",
            "[2021/05/07 07:40:32.499][INFO][optim.collaborative.step:219] Averaged tensors successfully with 8 peers\n",
            "05/07/2021 07:40:32 - INFO - __main__ -   Step 169\n",
            "05/07/2021 07:40:32 - INFO - __main__ -   Your current contribution: 252 samples\n",
            "05/07/2021 07:40:32 - INFO - __main__ -   Loss of your model: 10.945098360655736\n",
            "wandb: ERROR Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 500 encountered ({\"errors\":[{\"message\":\"Error 1040: Too many connections\",\"path\":[\"project\"]}],\"data\":{\"project\":null}}), retrying request\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:00:53.180491, resuming normal operation.\n",
            "[2021/05/07 07:41:55.177][INFO][optim.collaborative.step:219] Averaged tensors successfully with 8 peers\n",
            "05/07/2021 07:41:55 - INFO - __main__ -   Step 170\n",
            "05/07/2021 07:41:55 - INFO - __main__ -   Your current contribution: 704 samples\n",
            "05/07/2021 07:41:55 - INFO - __main__ -   Loss of your model: 10.951104385964916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93KCg3RZzEks"
      },
      "source": [
        "### What's up next?\n",
        "* Check the training progress on public learning curves: https://wandb.ai/yhn112/Demo-run-2/runs/1cnb06p7\n",
        "* Run a second GPU session with kaggle notebooks: **TBA**\n",
        "* See [this tutorial](https://github.com/learning-at-home/hivemind/tree/master/examples/albert) on how to start your own collaborative runs!\n",
        "\n",
        "\n",
        "_Co-created by [leshanbog](https://github.com/leshanbog), [yhn112](https://github.com/yhn112) and [foksly](https://github.com/foksly) from [hivemind](https://github.com/learning-at-home/hivemind) (YSDA), [lhoestq](https://github.com/lhoestq), [SaulLu](https://github.com/SaulLu) and [stas00@](https://github.com/stas00) from [huggingface](http://huggingface.co)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUQ_j06kS6pb"
      },
      "source": [
        "### How it works\n",
        "\n",
        "Since peers can join and leave at any time, we can't use global [Ring All-Reduce](https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da) for averaging: a single missing peer can break the entire protocol. Instead, peers dynamically assemble into small groups and run all-reduce within each group. Consider an example with 9 GPUs:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/QcD1mfG.png\" width=360px><br>\n",
        "The All-Reduce protocol within group can be Ring-AllReduce, but we use a simpler all-to-all algorithm known as butterfly-like all-reduce.<br>\n",
        "<img src=\"https://i.imgur.com/ewq3vS6.png\" width=380px><br>\n",
        "After each successful round, participants shuffle around and find new groups:<br>\n",
        "<img src=\"https://i.imgur.com/dexNCL3.png\" width=350px>\n",
        "\n",
        "If one of the peers fails to do his part, it will only affect his local group, and only for a single round.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/RBmElUi.png\" width=340px>\n",
        "\n",
        "Afterwards, peers from the failed group will find new groupmates according to the [moshpit algorithm](https://arxiv.org/abs/2103.03239).\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "If you want to learn more and even host your own collaborative experiments, take a look at the [hivemind library](https://github.com/learning-at-home/hivemind/) or the [Moshpit-SGD paper](https://arxiv.org/pdf/2103.03239.pdf).\n",
        "\n",
        "\n"
      ]
    }
  ]
}
