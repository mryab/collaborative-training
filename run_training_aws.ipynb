{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# the code below assumes that you configure boto3 with your AWS account\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html\n",
    "ec2 = boto3.resource('ec2')\n",
    "client = boto3.client('ec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"New_reference\"\n",
    "use_internal_routing = True\n",
    "\n",
    "# ^-- must be unique per experiment\n",
    "coordinator_type = \"c5.large\"\n",
    "dht_port = 31337\n",
    "worker_type = \"g4dn.xlarge\"\n",
    "num_workers = 16\n",
    "\n",
    "image_id = \"ami-0db67995cd75f5a9f\"\n",
    "aws_key_name = \"aws\"  ## update with your aws key name\n",
    "subnet = \"subnet-fcd1ca86\"  ## update with your subnet name or skip entirely\n",
    "security_group = \"sg-a75591d4\"  ## you guessed it\n",
    "\n",
    "data_path = \"https://hivemind-data.s3.us-east-2.amazonaws.com/wikitext103.tar.gz\"\n",
    "hivemind_version = \"master\" # branch, commit or tag for git checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the experiment name is unique.\n",
    "# disable this if you want to add more instances to an existing experiment\n",
    "existing_instances = ec2.instances.filter(Filters=[\n",
    "    {'Name': 'instance-state-name', 'Values': ['running']},\n",
    "    {'Name': 'tag:experiment', 'Values': [experiment_name]},\n",
    "])\n",
    "ins = list(existing_instances)\n",
    "if ins:\n",
    "    print(f\"Already running {experiment_name}: {ins}\")\n",
    "    print(len(ins))\n",
    "    for i in ins:\n",
    "        print(i.public_ip_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to remove all instances and spot requests, run this:\n",
    "existing_instances.terminate()\n",
    "requests_to_shutdown = []\n",
    "for request in client.describe_spot_instance_requests()['SpotInstanceRequests']:\n",
    "    if request['State'] == 'active' and any(\n",
    "        tag['Key'] == 'experiment' and tag['Value'] == experiment_name\n",
    "        for tag in request['Tags']):\n",
    "        requests_to_shutdown.append(request['SpotInstanceRequestId'])\n",
    "if requests_to_shutdown:\n",
    "    client.cancel_spot_instance_requests(\n",
    "        SpotInstanceRequestIds=requests_to_shutdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: run coordinator\n",
    "\n",
    "Coordinator is an instance that welcomes new peers into a decentralized training run. If coordinator is down, new peers can still join by initializing with one of the existing peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WandB_API_key = \"7cc938e45e63ef7d2f88f811be240ba0395c02dd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_script = f'''#!/bin/bash -ex\n",
    "exec > >(tee /var/log/user-command.log|logger -t user-data -s 2>/dev/console) 2>&1\n",
    "\n",
    "\n",
    "\n",
    "# note: we configure rsyslog to forward logs from all trainers\n",
    "sudo sh -c 'cat <<\"EOF\" >> /etc/rsyslog.conf\n",
    "$ModLoad imudp\n",
    "$UDPServerRun 514\n",
    "\n",
    "$ModLoad imtcp\n",
    "$InputTCPServerRun 514\n",
    "\n",
    "$FileCreateMode 0644\n",
    "$DirCreateMode 0755\n",
    "\n",
    "$template RemoteLogs,\"/var/log/rsyslog/%HOSTNAME%.log\"\n",
    "*.*  ?RemoteLogs\n",
    "& ~\n",
    "EOF'\n",
    "sudo systemctl restart rsyslog\n",
    "\n",
    "\n",
    "# NOTE: docker run must be called without --it as there is no tty\n",
    "# check machine's /var/log/user-command.log for details\n",
    "\n",
    "docker run --name trainer_run --ipc=host --net=host mrbn/hivemind bash -c \"\"\"\n",
    "set -euxo pipefail\n",
    "\n",
    "pip install whatsmyip\n",
    "pip install torch-optimizer\n",
    "git clone https://github.com/learning-at-home/hivemind\n",
    "cd hivemind\n",
    "git checkout {hivemind_version}\n",
    "pip install -e .\n",
    "\n",
    "pip install wandb\n",
    "\n",
    "sh -c 'cat <<\"EOF\" >> ~/.netrc\n",
    "machine api.wandb.ai\n",
    "  login user\n",
    "  password {WandB_API_key}\n",
    "EOF'\n",
    "\n",
    "cd examples/albert\n",
    "python ./run_first_peer.py --listen_on [::]:{dht_port}  --experiment_prefix {experiment_name} --wandb_project Demo-run-2\n",
    "\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator, = ec2.create_instances(\n",
    "    ImageId=image_id, InstanceType=coordinator_type,\n",
    "    MinCount=1, MaxCount=1,\n",
    "    SecurityGroupIds=[security_group], SubnetId=subnet,\n",
    "    KeyName=aws_key_name, UserData=coordinator_script,\n",
    "    TagSpecifications=[{'ResourceType': 'instance', 'Tags': [\n",
    "        {'Key':'experiment', 'Value': experiment_name},\n",
    "        {'Key':'role', 'Value': 'first_peer'}\n",
    "    ]}]\n",
    ")\n",
    "coordinator.wait_until_running()\n",
    "coordinator, = list(ec2.instances.filter(InstanceIds=[coordinator.id]))\n",
    "\n",
    "print(coordinator.private_ip_address, coordinator.public_ip_address)\n",
    "\n",
    "if use_internal_routing:\n",
    "    coordinator_ip = coordinator.private_ip_address\n",
    "else:\n",
    "    coordinator_ip = coordinator.public_ip_address\n",
    "\n",
    "coordinator_endpoint = f\"{coordinator_ip}:{dht_port}\"\n",
    "print(coordinator_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hivemind as src\n",
    "\n",
    "probe = await src.DHTNode.create(listen=False)\n",
    "for i in range(20):\n",
    "    ping_response = await probe.protocol.call_ping(f\"{coordinator.public_ip_address}:{dht_port}\")\n",
    "    if ping_response is not None:\n",
    "        print(\"Coordinator is now accessible to workers!\")\n",
    "        print(f\"Use public ip: {coordinator_endpoint}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Coordinator is not accessible yet, will retry in 30s...\")\n",
    "        time.sleep(30)\n",
    "else:\n",
    "    print(\"Coordinator failed to launch for some reason.\")\n",
    "    print(f\"Check /var/log/user-command.log at ec2-user@{coordinator.public_ip_address}\")\n",
    "    \n",
    "# this should normally take 3-6 minutes depending on the will of Bezos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: run workers\n",
    "\n",
    "Workers are preemptible GPU instances that run compute gradients and perform Moshpit averaging. In this example, each worker is a single tesla T4 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_script = f'''#!/bin/bash -ex\n",
    "exec > >(tee /var/log/user-command.log|logger -t user-data -s 2>/dev/console) 2>&1\n",
    "\n",
    "set -euxo pipefail\n",
    "cd ~\n",
    "\n",
    "docker run --name hivemind_run --gpus all --ipc=host --net=host mrbn/hivemind bash -c \"\"\"\n",
    "\n",
    "pip install torch-optimizer==0.1.0 scipy==1.5.2\n",
    "git clone https://github.com/learning-at-home/hivemind\n",
    "cd hivemind\n",
    "git checkout {hivemind_version}\n",
    "pip install -e .\n",
    "cd examples/albert\n",
    "\n",
    "ulimit -n 4096\n",
    "\n",
    "mkdir -p ~/data\n",
    "wget -qO- {data_path} | tar xzf -\n",
    "\n",
    "HIVEMIND_THREADS=256 python run_trainer.py \\\n",
    "  --output_dir ./outputs --overwrite_output_dir \\\n",
    "  --logging_dir ./logs --logging_first_step --logging_steps 100 \\\n",
    "  --initial_peers {coordinator_endpoint} --experiment_prefix {experiment_name} --seed 42\n",
    "\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    existing_instances = list(ec2.instances.filter(Filters=[\n",
    "        {'Name': 'instance-state-name', 'Values': ['running']},\n",
    "        {'Name': 'tag:experiment', 'Values': [experiment_name]},\n",
    "    ]))\n",
    "    \n",
    "    count_needed = num_workers+1-len(existing_instances)\n",
    "    if count_needed:\n",
    "        try:\n",
    "            print(\"CREATING ONE WORKER!\")\n",
    "            workers += ec2.create_instances(\n",
    "                ImageId=image_id, InstanceType=worker_type,\n",
    "                MinCount=1, MaxCount=1,\n",
    "                UserData=worker_script,\n",
    "                SecurityGroupIds=[security_group], SubnetId=\"subnet-36b11c5d\", \n",
    "                KeyName=aws_key_name,\n",
    "                InstanceMarketOptions={\n",
    "                    \"MarketType\": \"spot\",\n",
    "                    \"SpotOptions\": {\n",
    "                        \"SpotInstanceType\": \"one-time\",\n",
    "                        \"InstanceInterruptionBehavior\": \"terminate\"\n",
    "                    }\n",
    "                },\n",
    "                TagSpecifications=[{'ResourceType': 'instance', 'Tags': [\n",
    "                    {'Key':'experiment', 'Value': experiment_name},\n",
    "                    {'Key':'role', 'Value': 'gpu_worker'}\n",
    "                ]}, {'ResourceType': 'spot-instances-request', 'Tags': [\n",
    "                    {'Key':'experiment', 'Value': experiment_name},\n",
    "                    {'Key':'role', 'Value': 'gpu_worker'}\n",
    "                ]}],\n",
    "            )\n",
    "\n",
    "        except BaseException as e:\n",
    "            print(\"FAILED\", e)\n",
    "        else:\n",
    "            print(\"Added new peer\", workers[-1].public_ip_address)\n",
    "    else:\n",
    "        print(\"Enough workers already, check back in 60s...\")\n",
    "    time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
