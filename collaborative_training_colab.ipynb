{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collaborative-training-v0.14.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQIcM112KfI"
      },
      "source": [
        "<center><img src=\"https://i.imgur.com/FHMoW3N.png\" width=360px><br><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collaborative training <sup>v0.9 alpha</sup></b></center>\n",
        "\n",
        "\n",
        "This notebook will use local or colab GPU to help train ALBERT-large collaboratively. Your instance will compute gradients and exchange them with a bunch of volunteers around the world. We explain how it works at the bottom. But for now, please run all cells :)\n",
        "\n",
        "This is a test run to check if we have fixed the hanging issues from __v11__, found and reported by YSDA students. The run will be terminated by __23:59 28 april GMT+0__. Please do not run colab notebooks from multiple google accounts to comply with colab's EULA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUlL47uTK7DN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fb6b01-6ad9-45c1-eaed-0db6f7977c33"
      },
      "source": [
        "!pip install transformers datasets sentencepiece torch_optimizer==0.1.0\n",
        "!git clone https://github.com/learning-at-home/hivemind -b averager_qual2\n",
        "!cd hivemind && pip install -e .\n",
        "!curl -L https://hivemind-data.s3.us-east-2.amazonaws.com/wikitext103.tar.gz | tar xzf -\n",
        "\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"GPU device not found. If running in colab, please retry in a few minutes.\"\n",
        "device_name = torch.cuda.get_device_name(0)\n",
        "microbatch_size = 4 if device_name.endswith('T4') or device_name.endswith('P100') else 1\n",
        "print(f\"Running with device {device_name}, local batch size = {microbatch_size}\")\n",
        "\n",
        "!ulimit -n 4096 && HIVEMIND_THREADS=256 python ./hivemind/examples/albert/run_trainer.py \\\n",
        " --client_mode --initial_peers 18.224.82.24:31337 --averaging_expiration 10 --statistics_expiration 120 \\\n",
        " --batch_size_lead 200 --per_device_train_batch_size {microbatch_size} --gradient_accumulation_steps 1 \\\n",
        " --logging_first_step --logging_steps 100  --output_dir ./outputs --overwrite_output_dir --logging_dir ./logs \\\n",
        " --experiment_prefix albert-wikitext-v11 --seed 42"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 5.4MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d6/a3d2c55b940a7c556e88f5598b401990805fc0f0a28b2fc9870cf0b8c761/datasets-1.6.0-py3-none-any.whl (202kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 17.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 17.5MB/s \n",
            "\u001b[?25hCollecting torch_optimizer==0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0f/bc49a0f714a1896b80f31db9ba82eebcb2bad9e0f5757184574f8ecfe2f1/torch_optimizer-0.1.0-py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 44.4MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer==0.1.0) (1.8.1+cu101)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, huggingface-hub, fsspec, xxhash, datasets, sentencepiece, pytorch-ranger, torch-optimizer\n",
            "Successfully installed datasets-1.6.0 fsspec-2021.4.0 huggingface-hub-0.0.8 pytorch-ranger-0.1.1 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.10.2 torch-optimizer-0.1.0 transformers-4.5.1 xxhash-2.0.2\n",
            "Cloning into 'hivemind'...\n",
            "remote: Enumerating objects: 3882, done.\u001b[K\n",
            "remote: Counting objects: 100% (1424/1424), done.\u001b[K\n",
            "remote: Compressing objects: 100% (580/580), done.\u001b[K\n",
            "remote: Total 3882 (delta 892), reused 1149 (delta 737), pack-reused 2458\u001b[K\n",
            "Receiving objects: 100% (3882/3882), 2.37 MiB | 4.09 MiB/s, done.\n",
            "Resolving deltas: 100% (2515/2515), done.\n",
            "Obtaining file:///content/hivemind\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (3.13)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (1.4.1)\n",
            "Requirement already satisfied: prefetch_generator>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (1.0.1)\n",
            "Requirement already satisfied: msgpack>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (1.0.2)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (2.3.0)\n",
            "Collecting uvloop>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/05/805df4850d9659efd69d00076269ae6adcb0e151d1922cff822ead2c432a/uvloop-0.15.2-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 5.0MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.33.2\n",
            "  Using cached https://files.pythonhosted.org/packages/2c/8c/c4767d8f4e88a46983896ce3db52e972cbb9a9abc87a1625cbbc46c434ca/grpcio-1.37.0-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Collecting grpcio-tools>=1.33.2\n",
            "  Using cached https://files.pythonhosted.org/packages/43/7e/45ce1a5d2f5f84991323bf375cec826d07a1dd15f64a4c74c3e67839cd9c/grpcio_tools-1.37.0-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from hivemind==0.9.7) (3.12.4)\n",
            "Collecting configargparse>=1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting cryptography>=3.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 28.7MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a3/0ffdb6c63f45f10d19b8e8b32670b22ed089cafb29732f6bf8ce518821fb/pydantic-1.8.1-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->hivemind==0.9.7) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.33.2->hivemind==0.9.7) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from grpcio-tools>=1.33.2->hivemind==0.9.7) (56.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.4.6->hivemind==0.9.7) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.4.6->hivemind==0.9.7) (2.20)\n",
            "Building wheels for collected packages: configargparse\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=e681f5912f1688e741ed6bad7c83a9772244e709ff70a66c45335c9c5b558ff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n",
            "Successfully built configargparse\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: uvloop, grpcio, grpcio-tools, configargparse, cryptography, pydantic, hivemind\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Running setup.py develop for hivemind\n",
            "Successfully installed configargparse-1.4 cryptography-3.4.7 grpcio-1.37.0 grpcio-tools-1.37.0 hivemind pydantic-1.8.1 uvloop-0.15.2\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  898M  100  898M    0     0  14.9M      0  0:01:00  0:01:00 --:--:-- 16.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93KCg3RZzEks"
      },
      "source": [
        "### What's up next?\n",
        "* Check the training progress on public learning curves: https://wandb.ai/yhn112/Demo-run/runs/3mfdz8aw\n",
        "* Run a second GPU session with kaggle notebooks: https://www.kaggle.com/yhn112/collaborative-training-d87a28 \n",
        "* See [this tutorial](https://github.com/learning-at-home/hivemind/tree/master/examples/albert) on how to start your own collaborative runs!\n",
        "\n",
        "\n",
        "_Co-created by [leshanbog](https://github.com/leshanbog), [yhn112](https://github.com/yhn112) and [foksly](https://github.com/foksly) from [hivemind](https://github.com/learning-at-home/hivemind) (YSDA), [lhoestq](https://github.com/lhoestq), [SaulLu](https://github.com/SaulLu) and [stas00@](https://github.com/stas00) from [huggingface](http://huggingface.co)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUQ_j06kS6pb"
      },
      "source": [
        "### How it works\n",
        "\n",
        "Since peers can join and leave at any time, we can't use global [Ring All-Reduce](https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da) for averaging: a single missing peer can break the entire protocol. Instead, peers dynamically assemble into small groups and run all-reduce within each group. Consider an example with 9 GPUs:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/QcD1mfG.png\" width=360px><br>\n",
        "The All-Reduce protocol within group can be Ring-AllReduce, but we use a simpler all-to-all algorithm known as butterfly-like all-reduce.<br>\n",
        "<img src=\"https://i.imgur.com/ewq3vS6.png\" width=380px><br>\n",
        "After each successful round, participants shuffle around and find new groups:<br>\n",
        "<img src=\"https://i.imgur.com/dexNCL3.png\" width=350px>\n",
        "\n",
        "If one of the peers fails to do his part, it will only affect his local group, and only for a single round.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/RBmElUi.png\" width=340px>\n",
        "\n",
        "Afterwards, peers from the failed group will find new groupmates according to the [moshpit algorithm](https://arxiv.org/abs/2103.03239).\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "If you want to learn more and even host your own collaborative experiments, take a look at the [hivemind library](https://github.com/learning-at-home/hivemind/) or the [Moshpit-SGD paper](https://arxiv.org/pdf/2103.03239.pdf).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJAHgQMPWBeP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}