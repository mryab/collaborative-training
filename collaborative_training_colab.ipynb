{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdQIcM112KfI"
   },
   "source": [
    "<center><img src=\"https://i.imgur.com/FHMoW3N.png\" width=360px><br><b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Collaborative training <sup>v0.9 alpha</sup></b></center>\n",
    "\n",
    "\n",
    "This notebook will use local or colab GPU to help train ALBERT-large collaboratively. Your instance will compute gradients and exchange them with a bunch of volunteers around the world. We explain how it works at the bottom. But for now, please run all cells :)\n",
    "\n",
    "This is a test run to check if we have fixed the hanging issues from __v11__, found and reported by YSDA students. The run will be terminated by __23:59 28 april GMT+0__. Please do not run colab notebooks from multiple google accounts to comply with colab's EULA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUlL47uTK7DN",
    "outputId": "c4fb6b01-6ad9-45c1-eaed-0db6f7977c33"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentencepiece torch_optimizer==0.1.0\n",
    "!git clone https://github.com/learning-at-home/hivemind -b averager_qual2\n",
    "!cd hivemind && pip install -e .\n",
    "!curl -L https://hivemind-data.s3.us-east-2.amazonaws.com/wikitext103.tar.gz | tar xzf -\n",
    "\n",
    "\n",
    "from runner import run_with_logging\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"GPU device not found. If running in colab, please retry in a few minutes.\"\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "microbatch_size = 4 if device_name.endswith('T4') or device_name.endswith('P100') else 1\n",
    "print(f\"Running with device {device_name}, local batch size = {microbatch_size}\")\n",
    "\n",
    "coordinator_ip = \"18.224.82.24\"\n",
    "\n",
    "\n",
    "command = f\"\"\"!ulimit -n 4096 && HIVEMIND_THREADS=256 python ./hivemind/examples/albert/run_trainer.py \\\n",
    " --client_mode --initial_peers {coordinator_ip}:31337 --averaging_expiration 10 --statistics_expiration 120 \\\n",
    " --batch_size_lead 200 --per_device_train_batch_size {microbatch_size} --gradient_accumulation_steps 1 \\\n",
    " --logging_first_step --logging_steps 100  --output_dir ./outputs --overwrite_output_dir --logging_dir ./logs \\\n",
    " --experiment_prefix albert-wikitext-v11 --seed 42\"\"\"\n",
    "run_with_logging(command, coordinator_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93KCg3RZzEks"
   },
   "source": [
    "### What's up next?\n",
    "* Check the training progress on public learning curves: https://wandb.ai/yhn112/Demo-run/runs/3mfdz8aw\n",
    "* Run a second GPU session with kaggle notebooks: https://www.kaggle.com/yhn112/collaborative-training-d87a28 \n",
    "* See [this tutorial](https://github.com/learning-at-home/hivemind/tree/master/examples/albert) on how to start your own collaborative runs!\n",
    "\n",
    "\n",
    "_Co-created by [leshanbog](https://github.com/leshanbog), [yhn112](https://github.com/yhn112) and [foksly](https://github.com/foksly) from [hivemind](https://github.com/learning-at-home/hivemind) (YSDA), [lhoestq](https://github.com/lhoestq), [SaulLu](https://github.com/SaulLu) and [stas00@](https://github.com/stas00) from [huggingface](http://huggingface.co)_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUQ_j06kS6pb"
   },
   "source": [
    "### How it works\n",
    "\n",
    "Since peers can join and leave at any time, we can't use global [Ring All-Reduce](https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da) for averaging: a single missing peer can break the entire protocol. Instead, peers dynamically assemble into small groups and run all-reduce within each group. Consider an example with 9 GPUs:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/QcD1mfG.png\" width=360px><br>\n",
    "The All-Reduce protocol within group can be Ring-AllReduce, but we use a simpler all-to-all algorithm known as butterfly-like all-reduce.<br>\n",
    "<img src=\"https://i.imgur.com/ewq3vS6.png\" width=380px><br>\n",
    "After each successful round, participants shuffle around and find new groups:<br>\n",
    "<img src=\"https://i.imgur.com/dexNCL3.png\" width=350px>\n",
    "\n",
    "If one of the peers fails to do his part, it will only affect his local group, and only for a single round.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/RBmElUi.png\" width=340px>\n",
    "\n",
    "Afterwards, peers from the failed group will find new groupmates according to the [moshpit algorithm](https://arxiv.org/abs/2103.03239).\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "If you want to learn more and even host your own collaborative experiments, take a look at the [hivemind library](https://github.com/learning-at-home/hivemind/) or the [Moshpit-SGD paper](https://arxiv.org/pdf/2103.03239.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJAHgQMPWBeP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "collaborative-training-v0.14.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
